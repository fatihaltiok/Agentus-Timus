# agent/visual_agent.py
# -*- coding: utf-8 -*-
"""
VisualAgent v2.1 V4.5 - UI-Automation mit Claude Vision + SoM + Mouse Feedback.

Features:
- Claude Sonnet 4.5 f√ºr Vision & Kontext-Verst√§ndnis
- Set-of-Mark (SoM) f√ºr Grob-Lokalisierung
- Mouse Feedback Tool f√ºr Fein-Lokalisierung (NEU!)
- Cursor-Typ als Echtzeit-Feedback
- Automatische Textfeld-Suche bei ungenauen Koordinaten
- Multi-Monitor Support
- Verification nach Aktionen
- Loop-Detection

Architektur:
  Claude Vision ‚Üí Versteht Kontext, plant Aktionen
  SoM Tool ‚Üí Grob-Koordinaten (¬±50px)
  Mouse Feedback ‚Üí Fein-Koordinaten via Cursor-Typ (¬±5px)
  Moondream ‚Üí GPU-beschleunigte Objekterkennung

Version: 2.1 (Claude Sonnet 4.5 + SoM + Mouse Feedback)
"""

import logging
import os
import json
import asyncio
import base64
import io
import sys
import re
import time
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
from dataclasses import dataclass, field

# --- Modulpfad-Korrektur ---
CURRENT_SCRIPT_PATH = Path(__file__).resolve()
PROJECT_ROOT = CURRENT_SCRIPT_PATH.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# --- Imports ---
from dotenv import load_dotenv
import httpx
from openai import OpenAI
from utils.openai_compat import prepare_openai_params

# Shared Utilities
from agent.shared.screenshot import capture_screenshot_base64 as _shared_screenshot_b64
from agent.shared.mcp_client import MCPClient as _SharedMCPClient
from agent.shared.action_parser import parse_action as _shared_parse_action

# Screenshot Libraries
try:
    import mss
    from PIL import Image
    MSS_AVAILABLE = True
except ImportError:
    MSS_AVAILABLE = False

# --- Konfiguration ---
load_dotenv(dotenv_path=PROJECT_ROOT / ".env", override=True)

MCP_URL = os.getenv("MCP_URL", "http://127.0.0.1:5000")
DEBUG = os.getenv("VISUAL_AGENT_DEBUG", "1") == "1"
ACTIVE_MONITOR = int(os.getenv("ACTIVE_MONITOR", "1"))

# Claude Konfiguration
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")
VISION_MODEL = os.getenv("VISION_MODEL", "moondream-local")
USE_MOONDREAM = VISION_MODEL.lower().startswith("moondream")

# OpenAI Planung (f√ºr Moondream-Backend)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
VISUAL_PLANNING_MODEL = os.getenv("VISUAL_PLANNING_MODEL", "gpt-5-mini")

# Mouse Feedback Konfiguration
USE_MOUSE_FEEDBACK = os.getenv("USE_MOUSE_FEEDBACK", "1") == "1"
REFINE_RADIUS = int(os.getenv("MOUSE_REFINE_RADIUS", "80"))

if not USE_MOONDREAM and not ANTHROPIC_API_KEY:
    raise RuntimeError(
        "ANTHROPIC_API_KEY fehlt in der Umgebung.\n"
        "Setze ANTHROPIC_API_KEY in .env"
    )

if USE_MOONDREAM and not OPENAI_API_KEY:
    raise RuntimeError(
        "OPENAI_API_KEY fehlt in der Umgebung (Moondream-Backend).\n"
        "Setze OPENAI_API_KEY in .env"
    )

# --- Logging ---
log = logging.getLogger("visual_agent")
if not log.hasHandlers():
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s')
    handler.setFormatter(formatter)
    log.addHandler(handler)
    log.setLevel(logging.DEBUG if DEBUG else logging.INFO)

backend_label = "moondream" if USE_MOONDREAM else "anthropic"
log.info(f"üëÅÔ∏è VisualAgent v2.1 | Modell: {VISION_MODEL} | Backend: {backend_label}")
log.info(f"   Monitor: {ACTIVE_MONITOR} | Mouse Feedback: {USE_MOUSE_FEEDBACK}")

# --- System Prompt ---
VISUAL_SYSTEM_PROMPT = """
Du bist ein pr√§ziser visueller Automatisierungs-Agent f√ºr Computer-Steuerung.

# DEINE ARCHITEKTUR (3-Stufen Pr√§zision)

```
Stufe 1: SoM Tool        ‚Üí Findet Elemente, gibt GROBE Koordinaten (¬±50px)
Stufe 2: Mouse Feedback  ‚Üí Verfeinert zu EXAKTEN Koordinaten via Cursor-Typ
Stufe 3: Click & Type    ‚Üí F√ºhrt Aktion aus
```

# KRITISCHE REGELN

## 0. NIEMALS OHNE AKTION BEENDEN! ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
VERBOTEN: finish_task() in Iteration 1 aufrufen!
VERBOTEN: Behaupten etwas getan zu haben ohne Tool-Aufruf!
VERBOTEN: Den Screenshot als "schon erledigt" interpretieren!

DU MUSST:
‚úì IMMER mindestens ein Tool aufrufen (au√üer finish_task)
‚úì Schrittweise vorgehen: scan ‚Üí click ‚Üí type ‚Üí verify
‚úì Nur finish_task wenn Observation zeigt "success": true

BEISPIEL FALSCH:
Iteration 1: {"action": {"method": "finish_task", "params": {...}}}  ‚ùå VERBOTEN!

BEISPIEL RICHTIG:
Iteration 1: {"action": {"method": "start_visual_browser", "params": {...}}}  ‚úì
Iteration 2: {"action": {"method": "scan_ui_elements", "params": {...}}}  ‚úì
...
Iteration N: {"action": {"method": "finish_task", "params": {...}}}  ‚úì

## 1. IMMER Mouse Feedback f√ºr Textfelder nutzen!
FALSCH: scan_ui_elements ‚Üí click_at(500, 300) ‚Üí Miss!
RICHTIG: scan_ui_elements ‚Üí refine_and_click(500, 300, "text_field") ‚Üí Treffer!

## 2. Der Cursor-Typ verr√§t dir ALLES
- "ibeam" (‚å∂) = Textfeld! Hier kannst du tippen
- "hand" (üëÜ) = Klickbar! Link oder Button
- "arrow" (‚ûú) = Normaler Bereich, nichts interaktives
- "wait" (‚è≥) = Laden, warte ab

## 3. Nach erfolgreichem Klick auf Textfeld ‚Üí SOFORT type_text()
Wenn cursor_type="ibeam" nach Klick:
‚Üí N√§chster Schritt MUSS type_text() sein!
‚Üí NIEMALS erneut scannen!

## 4. Bei "nicht gefunden" ‚Üí Radius erh√∂hen oder anderen Ansatz
Wenn find_text_field_nearby fehlschl√§gt:
‚Üí Vergr√∂√üere radius auf 120 oder 150
‚Üí Oder nutze search_for_element mit element_type="any"

# STANDARD-WORKFLOW (Schrittweise!)

‚ö†Ô∏è WICHTIG: Jeder Schritt = Eine Aktion = Ein Tool-Aufruf!

**Aufgabe: "Gehe zu ChatGPT und frage XYZ"**

**Schritt 1 - Browser √∂ffnen (FALLS N√ñTIG):**
```json
{"thought": "Kein Browser sichtbar. √ñffne Browser mit ChatGPT URL.", "action": {"method": "start_visual_browser", "params": {"url": "https://chatgpt.com"}}}
```

**Schritt 2 - UI scannen (nach Browser-Start):**
```json
{"thought": "Browser offen. Scanne nach Chat-Eingabefeld.", "action": {"method": "scan_ui_elements", "params": {"element_types": ["text field", "chat input", "message box"]}}}
```

**Schritt 3 - Klicken:**
```json
{"thought": "Feld bei (500, 300) gefunden. Klicke sofort.", "action": {"method": "click_immediately", "params": {"x": 500, "y": 300}}}
```

**Schritt 4 - Text eingeben:**
```json
{"thought": "Klick erfolgreich. Tippe Frage direkt ein (ohne Zwischenablage).", "action": {"method": "type_text", "params": {"text_to_type": "Was ist die Hauptstadt von Frankreich?", "press_enter_after": true, "method": "write"}}}
```

**WICHTIG: Bei ChatGPT/Web-Interfaces ‚Üí IMMER method="write" verwenden!**
- "write" = Direktes Tippen (sichtbar, robust, funktioniert ohne perfekten Fokus)
- Standard (ohne method) = Zwischenablage (schnell, aber erfordert perfekten Fokus)

**Schritt 5 - Fertig:**
```json
{"thought": "Frage gesendet. Aufgabe erledigt.", "action": {"method": "finish_task", "params": {"message": "Frage an ChatGPT gesendet"}}}
```

# VERF√úGBARE TOOLS

## SoM (Set-of-Mark) - Grob-Lokalisierung
- scan_ui_elements(element_types) ‚Üí Findet [1], [2], [3]...
- get_element_coordinates(element_id) ‚Üí x,y f√ºr ID
- find_and_click_element(element_type) ‚Üí Sucht und gibt Koordinaten

## Klick-Optionen (priorisiert nach Geschwindigkeit)
1. **click_immediately(x, y)** ‚Üí ‚ö° SCHNELLSTER Klick, f√ºr Buttons/Links
2. **click_and_focus(x, y)** ‚Üí üéØ ROBUSTER Klick (2x) f√ºr hartn√§ckige Eingabefelder (ChatGPT!)
3. **refine_and_click(x, y, element_type)** ‚Üí üîç Verfeinert Position (5s Timeout) + klickt
4. **click_at(x, y)** ‚Üí üñ±Ô∏è Einfacher Klick (letztes Mittel)

## Mouse Feedback - Erweiterte Suche (optional)
- find_text_field_nearby(x, y, radius) ‚Üí Sucht Textfeld via Cursor
- search_for_element(x, y, radius, element_type) ‚Üí Spiral-Suche
- move_with_feedback(x, y) ‚Üí Bewegt mit Cursor-Feedback
- get_cursor_at_position(x, y) ‚Üí Pr√ºft Cursor-Typ

## Tastatur & Scroll
- type_text(text_to_type, press_enter_after, method="write") ‚Üí Tippt Text
  ‚Ä¢ method="write" = Direktes Tippen Zeichen f√ºr Zeichen (EMPFOHLEN f√ºr ChatGPT/Web!)
  ‚Ä¢ method="clipboard" = Zwischenablage + Ctrl+V (schnell, braucht perfekten Fokus)
  ‚Ä¢ Ohne method = Auto (probiert Zwischenablage, Fallback zu write)
- scroll(direction, amount) ‚Üí Scrollt

## Browser & Apps
- start_visual_browser(url) ‚Üí Browser mit URL √∂ffnen
- open_application(app_name) ‚Üí App starten

# ELEMENT-TYPEN

F√ºr scan_ui_elements:
- Standard: button, text field, input field, search bar, icon, link
- Chat: chat input, message box, textbox, send button

F√ºr refine_and_click / search_for_element:
- text_field ‚Üí Sucht Cursor "ibeam"
- clickable ‚Üí Sucht Cursor "hand"
- any ‚Üí Jedes interaktive Element

# ANTWORTFORMAT
IMMER JSON: {"thought": "...", "action": {"method": "...", "params": {...}}}
NUR EINE Aktion pro Antwort!

# BEISPIEL: Chat-Eingabe in Gemini

User: "Schreibe 'Hallo' in das Gemini Chat-Feld"

Schritt 1:
{"thought": "Suche Chat-Eingabefeld", "action": {"method": "scan_ui_elements", "params": {"element_types": ["chat input", "text field", "textbox"]}}}

Observation: {"elements": [{"id": 3, "type": "chat input", "x": 895, "y": 517}]}

Schritt 2 (OPTION A - Schneller Klick):
{"thought": "Element [3] bei (895, 517). SoM ist meist pr√§zise, nutze SOFORT-KLICK.", "action": {"method": "click_immediately", "params": {"x": 895, "y": 517}}}

Schritt 2 (OPTION B - Wenn unsicher √ºber Position):
{"thought": "Element [3] bei (895, 517). Unsichere Position, verfeinere zuerst.", "action": {"method": "refine_and_click", "params": {"x": 895, "y": 517, "element_type": "text_field"}}}

Observation: {"success": true, "x": 895, "y": 517, "cursor_type": "ibeam", "clicked": true}

Schritt 3:
{"thought": "Klick erfolgreich, Cursor ist ibeam. Jetzt tippen!", "action": {"method": "type_text", "params": {"text_to_type": "Hallo", "press_enter_after": false}}}

Observation: {"success": true}

Schritt 4:
{"thought": "Text eingegeben. Fertig!", "action": {"method": "finish_task", "params": {"message": "Hallo wurde in das Gemini Chat-Feld eingegeben."}}}
"""

VISUAL_SYSTEM_PROMPT_TEXT = (
    VISUAL_SYSTEM_PROMPT
    + "\n\nDu erh√§ltst statt eines Bildes eine Textbeschreibung des Screens (Moondream). "
      "Nutze die Beschreibung und die bisherigen Observations, um den n√§chsten Tool-Schritt zu w√§hlen."
)


# --- Claude Vision Client ---
class ClaudeVisionClient:
    """Async Client f√ºr Claude Vision API."""
    
    def __init__(self):
        self.api_key = ANTHROPIC_API_KEY
        self.model = VISION_MODEL
        self.base_url = "https://api.anthropic.com/v1/messages"
    
    async def chat_with_image(
        self,
        messages: List[Dict[str, Any]],
        system: str = "",
        image_base64: Optional[str] = None,
        max_tokens: int = 1500,
        temperature: float = 0.1
    ) -> str:
        """Sendet Nachricht mit optionalem Bild an Claude."""
        
        headers = {
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json"
        }
        
        # Messages f√ºr Anthropic Format konvertieren
        converted_messages = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                continue
            
            if isinstance(content, list):
                converted_content = []
                for item in content:
                    if item.get("type") == "text":
                        converted_content.append({
                            "type": "text",
                            "text": item.get("text", "")
                        })
                    elif item.get("type") == "image_url":
                        url = item.get("image_url", {}).get("url", "")
                        if url.startswith("data:image"):
                            parts = url.split(",", 1)
                            media_type = parts[0].split(";")[0].replace("data:", "")
                            b64_data = parts[1] if len(parts) > 1 else ""
                            converted_content.append({
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": media_type,
                                    "data": b64_data
                                }
                            })
                converted_messages.append({"role": role, "content": converted_content})
            else:
                converted_messages.append({"role": role, "content": content})
        
        if image_base64 and converted_messages:
            last_msg = converted_messages[-1]
            if last_msg["role"] == "user":
                if isinstance(last_msg["content"], str):
                    last_msg["content"] = [
                        {"type": "text", "text": last_msg["content"]},
                        {
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/png",
                                "data": image_base64
                            }
                        }
                    ]
                elif isinstance(last_msg["content"], list):
                    last_msg["content"].append({
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": "image/png",
                            "data": image_base64
                        }
                    })
        
        payload = {
            "model": self.model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "system": system,
            "messages": converted_messages
        }
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    self.base_url,
                    headers=headers,
                    json=payload
                )
                response.raise_for_status()
                data = response.json()
                
                content = data.get("content", [])
                if content and isinstance(content, list):
                    return content[0].get("text", "").strip()
                return ""
                
        except httpx.TimeoutException:
            log.error("Claude Vision API Timeout")
            return '{"thought": "API Timeout", "action": {"method": "finish_task", "params": {"message": "API Timeout"}}}'
        except httpx.HTTPStatusError as e:
            log.error(f"Claude Vision HTTP Fehler: {e.response.status_code}")
            return f'{{"thought": "HTTP Error", "action": {{"method": "finish_task", "params": {{"message": "HTTP {e.response.status_code}"}}}}}}'
        except Exception as e:
            log.error(f"Claude Vision Fehler: {e}")
            return f'{{"thought": "Error", "action": {{"method": "finish_task", "params": {{"message": "{e}"}}}}}}'


class OpenAIPlannerClient:
    """Text-basierter Planner f√ºr Moondream-Backend."""

    def __init__(self):
        self.model = VISUAL_PLANNING_MODEL
        self.client = OpenAI(api_key=OPENAI_API_KEY)

    async def chat(self, messages: List[Dict[str, Any]], system: str = "") -> str:
        payload_messages: List[Dict[str, str]] = []
        if system:
            payload_messages.append({"role": "system", "content": system})
        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, list):
                content = " ".join([str(item.get("text", "")) for item in content if item.get("type") == "text"])
            payload_messages.append({"role": msg.get("role", "user"), "content": str(content)})

        params = prepare_openai_params({
            "model": self.model,
            "messages": payload_messages,
            "temperature": 0.1,
            "max_tokens": 1500,
            "response_format": {"type": "json_object"}
        })

        response = await asyncio.to_thread(self.client.chat.completions.create, **params)
        return (response.choices[0].message.content or "").strip()


# Globaler Client
claude_client = ClaudeVisionClient()
openai_planner_client = OpenAIPlannerClient() if USE_MOONDREAM else None


# --- Screenshot (delegiert an shared utility) ---
def get_screenshot_base64() -> str:
    """Erstellt Screenshot als Base64 (delegiert an agent.shared.screenshot)."""
    return _shared_screenshot_b64(monitor_index=ACTIVE_MONITOR, fmt="PNG")


# --- Tool-Aufruf (delegiert an shared MCPClient) ---
_mcp_client = _SharedMCPClient(timeout=60.0)

async def call_tool(method: str, params: Optional[dict] = None, timeout: int = 60) -> dict:
    """Async RPC zum MCP-Server (delegiert an agent.shared.mcp_client)."""
    params = params or {}
    log.info(f"Tool: {method} | {str(params)[:100]}")
    return await _mcp_client.call(method, params)


async def get_moondream_screen_description(task: str) -> str:
    """Erstellt eine Textbeschreibung des aktuellen Screens via Moondream."""
    question = (
        "Describe the visible UI for this task: '" + task + "'. "
        "List interactive elements (buttons, links, input fields) with their labels and approximate positions "
        "(top/middle/bottom, left/center/right). Keep it concise."
    )
    result = await call_tool("describe_ui_with_moondream", {"question": question})
    if result.get("error"):
        log.error(f"Moondream Beschreibung fehlgeschlagen: {result['error']}")
        return ""
    return str(result.get("description", "")).strip()


# --- Refine and Click (NEU!) ---
async def refine_and_click(x: int, y: int, element_type: str = "text_field", radius: int = REFINE_RADIUS) -> dict:
    """
    Kombiniert Mouse Feedback + Klick in einem Schritt.
    
    1. Nutzt find_text_field_nearby oder search_for_element
    2. Klickt wenn gefunden
    3. Gibt Cursor-Info zur√ºck
    
    Args:
        x: Grob-Koordinate X (von SoM)
        y: Grob-Koordinate Y (von SoM)
        element_type: "text_field", "clickable", "any"
        radius: Suchradius
    
    Returns:
        Refined position, cursor_type, click success
    """
    if not USE_MOUSE_FEEDBACK:
        # Fallback: Direkter Klick ohne Refinement
        result = await call_tool("click_at", {"x": x, "y": y})
        return {
            "success": not result.get("error"),
            "refined_x": x,
            "refined_y": y,
            "cursor_type": "unknown",
            "clicked": True,
            "method": "direct_click"
        }
    
    # Mouse Feedback nutzen (mit Timeout)
    try:
        if element_type == "text_field":
            result = await asyncio.wait_for(
                call_tool("find_text_field_nearby", {"x": x, "y": y, "radius": radius}),
                timeout=5.0  # Max 5 Sekunden f√ºr Suche
            )
        else:
            result = await asyncio.wait_for(
                call_tool("search_for_element", {
                    "center_x": x,
                    "center_y": y,
                    "radius": radius,
                    "element_type": element_type
                }),
                timeout=5.0  # Max 5 Sekunden f√ºr Suche
            )
    except asyncio.TimeoutError:
        log.warning(f"‚ö†Ô∏è Mouse Feedback Timeout nach 5s! Fallback zu direktem Klick.")
        result = {"found": False, "timeout": True}
    
    if result.get("found") or result.get("found_interactive"):
        refined_x = result.get("x", x)
        refined_y = result.get("y", y)
        cursor_type = result.get("cursor_type", "unknown")

        # Direkter Klick ohne Verifikation (schneller und zuverl√§ssiger)
        log.info(f"üéØ Refinement erfolgreich: ({refined_x}, {refined_y}), Cursor: {cursor_type}")
        log.info(f"üñ±Ô∏è F√ºhre DIREKTEN Klick aus (ohne Verifikation)")
        click_result = await call_tool("click_at", {"x": refined_x, "y": refined_y})

        # Kurze Pause nach Klick
        await asyncio.sleep(0.3)

        return {
            "success": True,
            "refined_x": refined_x,
            "refined_y": refined_y,
            "cursor_type": cursor_type,
            "clicked": not click_result.get("error"),
            "was_text_field": cursor_type == "ibeam",
            "was_clickable": cursor_type in ["ibeam", "hand"],
            "method": "mouse_feedback_direct_click"
        }
    else:
        # Nicht gefunden - trotzdem versuchen mit direktem Klick auf urspr√ºngliche Koordinaten
        log.warning(f"‚ö†Ô∏è Element nicht in Radius {radius}px gefunden oder Timeout.")
        log.info(f"üñ±Ô∏è FALLBACK: Direkter Klick auf urspr√ºngliche Koordinaten ({x}, {y})")

        click_result = await call_tool("click_at", {"x": x, "y": y})
        click_success = not click_result.get("error")

        # Kurze Pause
        await asyncio.sleep(0.3)

        # Cursor danach pr√ºfen
        cursor_result = await call_tool("get_cursor_at_position", {})

        return {
            "success": click_success,  # True wenn Klick erfolgreich war
            "refined_x": x,
            "refined_y": y,
            "cursor_type": cursor_result.get("cursor_type", "unknown"),
            "clicked": click_success,
            "was_text_field": cursor_result.get("is_text_field", False),
            "method": "fallback_direct_click",
            "message": f"Refinement fehlgeschlagen, direkter Klick auf ({x}, {y})"
        }


# --- Direct Click (NEU f√ºr schnelles Klicken) ---
async def click_immediately(x: int, y: int) -> dict:
    """
    Direkter Klick ohne Refinement oder Verifikation.

    Nutze dies wenn:
    - Die Koordinaten bereits pr√§zise sind
    - Schnelligkeit wichtiger ist als Pr√§zision
    - Mouse Feedback zu langsam ist

    Args:
        x: X-Koordinate
        y: Y-Koordinate

    Returns:
        Klick-Ergebnis
    """
    log.info(f"‚ö° SOFORT-KLICK auf ({x}, {y}) ohne Refinement")

    result = await call_tool("click_at", {"x": x, "y": y})
    success = not result.get("error")

    await asyncio.sleep(0.2)

    # Cursor-Status nach Klick
    cursor_result = await call_tool("get_cursor_at_position", {})

    return {
        "success": success,
        "x": x,
        "y": y,
        "clicked": success,
        "cursor_type": cursor_result.get("cursor_type", "unknown"),
        "method": "immediate_click"
    }


# --- Smart Action Executor ---
async def execute_smart_action(method: str, params: dict) -> dict:
    """
    F√ºhrt Aktion intelligent aus mit automatischem Mouse Feedback.

    Ersetzt einfache click_at durch refine_and_click wenn sinnvoll.
    """
    # Spezial-Handling f√ºr refine_and_click
    if method == "refine_and_click":
        return await refine_and_click(
            x=params.get("x", 0),
            y=params.get("y", 0),
            element_type=params.get("element_type", "text_field"),
            radius=params.get("radius", REFINE_RADIUS)
        )

    # Spezial-Handling f√ºr click_immediately
    if method == "click_immediately":
        return await click_immediately(
            x=params.get("x", 0),
            y=params.get("y", 0)
        )

    # Smart Click: Wenn click_at auf vermutetes Textfeld, nutze Refinement
    # DEAKTIVIERT: Zu aggressiv, f√ºhrt zu unerw√ºnschtem Verhalten
    # Stattdessen: Agent soll explizit refine_and_click oder click_immediately nutzen

    # Standard Tool-Aufruf
    return await call_tool(method, params)


# --- Verification Helpers ---
async def verify_action(method: str) -> Tuple[bool, str]:
    """Verifiziert ob Aktion erfolgreich war."""
    needs_verify = ["click_at", "type_text", "refine_and_click", "click_with_verification"]
    if method not in needs_verify:
        return True, "no_verification_needed"
    
    result = await call_tool("verify_action_result", {"timeout": 5.0})
    success = result.get("success", False)
    
    if success:
        change = result.get("change_percentage", 0)
        log.info(f"‚úÖ Verifiziert: {change:.1f}% √Ñnderung")
        return True, f"verified_{change:.0f}pct_change"
    else:
        msg = result.get("message", "unknown")
        log.warning(f"‚ö†Ô∏è Nicht verifiziert: {msg}")
        return False, msg


async def wait_stable():
    """Wartet auf UI-Stabilit√§t."""
    await call_tool("wait_until_stable", {"timeout": 3.0})


# --- Parsing (delegiert an shared utility) ---
def parse_action(text: str) -> Tuple[Optional[dict], Optional[str]]:
    """Extrahiert Action aus LLM-Antwort (delegiert an agent.shared.action_parser)."""
    return _shared_parse_action(text)


# --- Loop Detection ---
@dataclass
class ActionTracker:
    """Trackt Aktionen f√ºr Loop-Detection."""
    history: List[str] = field(default_factory=list)
    max_history: int = 20
    max_repeats: int = 3
    
    def add(self, method: str, params: dict) -> bool:
        """F√ºgt Aktion hinzu. Gibt True zur√ºck wenn Loop erkannt."""
        # Ignoriere bestimmte Parameter f√ºr Vergleich
        clean_params = {k: v for k, v in params.items() if not k.startswith("_")}
        key = f"{method}:{json.dumps(clean_params, sort_keys=True)}"
        
        count = self.history.count(key)
        if count >= self.max_repeats - 1:
            log.warning(f"‚ö†Ô∏è Loop erkannt ({count + 1}x): {method}")
            return True
        
        self.history.append(key)
        if len(self.history) > self.max_history:
            self.history.pop(0)
        
        return False
    
    def reset(self):
        """Reset nach erfolgreichem Schritt."""
        self.history = []


# --- Context Tracker ---
@dataclass
class ContextTracker:
    """Trackt Kontext f√ºr intelligente Entscheidungen."""
    last_scan_elements: List[dict] = field(default_factory=list)
    last_clicked_type: Optional[str] = None
    last_cursor_type: Optional[str] = None
    successful_clicks: int = 0
    failed_clicks: int = 0
    
    def update_from_scan(self, elements: List[dict]):
        """Aktualisiert nach scan_ui_elements."""
        self.last_scan_elements = elements
    
    def update_from_click(self, result: dict):
        """Aktualisiert nach Klick."""
        if result.get("success") or result.get("clicked"):
            self.successful_clicks += 1
            self.last_cursor_type = result.get("cursor_type")
            if result.get("was_text_field") or self.last_cursor_type == "ibeam":
                self.last_clicked_type = "text_field"
            elif result.get("was_clickable") or self.last_cursor_type == "hand":
                self.last_clicked_type = "clickable"
        else:
            self.failed_clicks += 1
    
    def should_type_next(self) -> bool:
        """Pr√ºft ob als n√§chstes getippt werden sollte."""
        return self.last_cursor_type == "ibeam" or self.last_clicked_type == "text_field"


# --- Haupt-Agent ---
async def run_visual_task(task: str, max_iterations: int = 30) -> str:
    """F√ºhrt eine visuelle Automatisierungs-Aufgabe aus."""
    log.info(f"üëÅÔ∏è VisualAgent v2.1 startet: {task}")
    log.info(f"   Mouse Feedback: {'AKTIV' if USE_MOUSE_FEEDBACK else 'DEAKTIVIERT'}")
    
    if not MSS_AVAILABLE:
        return "Fehler: mss/PIL nicht installiert. Screenshots nicht m√∂glich."
    
    # State
    history = [
        {"role": "user", "content": f"AUFGABE: {task}"}
    ]
    action_tracker = ActionTracker()
    context = ContextTracker()
    
    for iteration in range(max_iterations):
        log.info(f"\n{'='*60}")
        log.info(f"Iteration {iteration + 1}/{max_iterations}")
        log.info(f"{'='*60}")
        
        screen_description = ""
        if USE_MOONDREAM:
            screen_description = await get_moondream_screen_description(task)
            if not screen_description:
                return "Fehler: Moondream Screen-Beschreibung fehlgeschlagen"
        else:
            # Screenshot machen
            screenshot = await asyncio.to_thread(get_screenshot_base64)
            if not screenshot:
                return "Fehler: Screenshot nicht m√∂glich"
        
        # Kontext-Hinweise f√ºr LLM
        context_hints = []
        if context.should_type_next():
            context_hints.append("HINWEIS: Letzter Klick war auf Textfeld (ibeam). Jetzt type_text() nutzen!")
        if context.failed_clicks > 2:
            context_hints.append(f"HINWEIS: {context.failed_clicks} fehlgeschlagene Klicks. Versuche anderen Ansatz!")
        
        context_text = "\n".join(context_hints) if context_hints else ""
        
        # Aktuelle Nachricht
        user_content = "Aktueller Screenshot. N√§chster Schritt?"
        if USE_MOONDREAM:
            user_content = "Aktuelle Screen-Beschreibung. N√§chster Schritt?"
        if context_text:
            user_content = f"{context_text}\n\n{user_content}"

        if USE_MOONDREAM:
            current_msg = {
                "role": "user",
                "content": f"{user_content}\n\nSCREEN:\n{screen_description}"
            }
        else:
            current_msg = {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_content},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{screenshot}"}}
                ]
            }
        
        messages = history + [current_msg]
        
        # Planner aufrufen
        if USE_MOONDREAM:
            reply = await openai_planner_client.chat(
                messages=messages,
                system=VISUAL_SYSTEM_PROMPT_TEXT
            )
        else:
            reply = await claude_client.chat_with_image(
                messages=messages,
                system=VISUAL_SYSTEM_PROMPT,
                max_tokens=1500,
                temperature=0.1
            )
        
        log.debug(f"üß† Antwort: {reply[:300]}...")
        
        # Final Answer Check
        if "Final Answer:" in reply:
            return reply.split("Final Answer:")[1].strip()
        
        # Action parsen
        action, err = parse_action(reply)
        
        if not action:
            log.warning(f"Parse-Fehler: {err}")
            history.append({"role": "assistant", "content": reply})
            history.append({"role": "user", "content": f"Fehler: {err}. Sende g√ºltiges JSON."})
            continue
        
        method = action.get("method", "")
        params = action.get("params", {})

        log.info(f"üìã Action: {method}({params})")

        # Anti-Hallucination: Verhindere finish_task in Iteration 1
        if method == "finish_task" and iteration == 0:
            log.warning("‚ö†Ô∏è WARNUNG: finish_task in Iteration 1 ist VERBOTEN!")
            history.append({"role": "assistant", "content": reply})
            history.append({
                "role": "user",
                "content": "‚ùå FEHLER: Du kannst nicht in Iteration 1 die Aufgabe beenden!\n\n"
                          "Du MUSST zuerst Aktionen durchf√ºhren:\n"
                          "1. start_visual_browser(url) - Wenn Browser n√∂tig\n"
                          "2. scan_ui_elements() - Finde UI-Elemente\n"
                          "3. click_immediately() oder refine_and_click() - Klicke\n"
                          "4. type_text() - Gib Text ein\n"
                          "5. Erst DANN: finish_task()\n\n"
                          "Beginne JETZT mit Schritt 1!"
            })
            continue

        # finish_task Check (nur wenn nicht Iteration 1)
        if method == "finish_task":
            msg = params.get("message", "Aufgabe abgeschlossen")
            log.info(f"‚úÖ Aufgabe beendet: {msg}")
            return msg
        
        # Loop Detection
        if action_tracker.add(method, params):
            history.append({"role": "assistant", "content": reply})
            history.append({
                "role": "user",
                "content": "‚ö†Ô∏è Loop erkannt! Versuche eine ANDERE Strategie:\n"
                          "1. Nutze refine_and_click statt click_at\n"
                          "2. Vergr√∂√üere den radius Parameter\n"
                          "3. Nutze search_for_element mit element_type='any'"
            })
            continue

        # Before-Screenshot f√ºr Verifikation (bei bestimmten Aktionen)
        verification_methods = ["click_at", "type_text", "refine_and_click", "click_immediately", "click_with_verification"]
        if method in verification_methods:
            await call_tool("capture_screen_before_action", {})

        # Action ausf√ºhren
        result = await execute_smart_action(method, params)

        # Kontext aktualisieren
        if method == "scan_ui_elements":
            elements = result.get("elements", [])
            context.update_from_scan(elements)
        elif method in ["click_at", "refine_and_click", "click_with_verification"]:
            context.update_from_click(result)
        
        # Verification f√ºr wichtige Aktionen
        if method in ["click_at", "type_text", "refine_and_click"]:
            verified, verify_msg = await verify_action(method)
            
            if not verified and method != "type_text":
                context.failed_clicks += 1
                history.append({"role": "assistant", "content": reply})
                history.append({
                    "role": "user",
                    "content": f"‚ö†Ô∏è Aktion nicht verifiziert: {verify_msg}\n"
                              "Versuche:\n"
                              "1. refine_and_click mit gr√∂√üerem radius (120)\n"
                              "2. search_for_element zum Finden des Elements\n"
                              "3. Anderen Bereich des Bildschirms"
                })
                continue
            
            # Warte auf Stabilit√§t
            await wait_stable()
            
            # Reset Loop-Tracker nach erfolgreichem Schritt
            action_tracker.reset()
        
        # History aktualisieren
        history.append({"role": "assistant", "content": reply})
        
        # Observation formatieren
        obs_str = json.dumps(result, ensure_ascii=False)
        if len(obs_str) > 1000:
            obs_str = obs_str[:1000] + "..."
        
        # Zus√§tzliche Hinweise basierend auf Ergebnis
        hints = []
        if method == "refine_and_click" and result.get("was_text_field"):
            hints.append("‚úÖ Textfeld gefunden und geklickt! N√§chster Schritt: type_text()")
        if method == "scan_ui_elements" and result.get("count", 0) == 0:
            hints.append("‚ö†Ô∏è Keine Elemente gefunden. Versuche anderen element_type oder scroll.")
        
        obs_content = f"Observation: {obs_str}"
        if hints:
            obs_content += "\n\n" + "\n".join(hints)
        
        history.append({"role": "user", "content": obs_content})
        
        # Kurze Pause
        await asyncio.sleep(0.3)
    
    return f"Max Iterationen ({max_iterations}) erreicht."


# --- Sync Wrapper ---
def run_visual_task_sync(task: str, max_iterations: int = 30) -> str:
    """Sync Wrapper f√ºr run_visual_task."""
    return asyncio.run(run_visual_task(task, max_iterations))


# --- CLI ---
if __name__ == "__main__":
    if len(sys.argv) > 1:
        task = " ".join(sys.argv[1:])
        result = run_visual_task_sync(task)
        
        print("\n" + "=" * 80)
        print("üëÅÔ∏è FINALE ANTWORT DES VISUAL-AGENTEN:")
        print("=" * 80)
        print(result)
        print("=" * 80)
    else:
        print("\nüëÅÔ∏è Timus VisualAgent v2.1")
        print(f"   Modell: {VISION_MODEL}")
        print(f"   Monitor: {ACTIVE_MONITOR}")
        print(f"   Mouse Feedback: {'AKTIV' if USE_MOUSE_FEEDBACK else 'DEAKTIVIERT'}")
        print("\nFeatures:")
        print("  ‚úì Claude Sonnet 4.5 Vision")
        print("  ‚úì SoM f√ºr Grob-Lokalisierung")
        print("  ‚úì Mouse Feedback f√ºr Fein-Lokalisierung")
        print("  ‚úì Cursor-Typ als Echtzeit-Feedback")
        print("  ‚úì Auto-Refinement bei Textfeld-Klicks")
        print("\nBeispiele:")
        print("  python visual_agent.py \"√ñffne Firefox und gehe zu google.com\"")
        print("  python visual_agent.py \"Schreibe 'Hallo' in das Chat-Feld\"")
        print("  python visual_agent.py \"Klicke auf den Login-Button\"")
